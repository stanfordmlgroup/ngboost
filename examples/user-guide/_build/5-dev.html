---
interact_link: content/5-dev.ipynb
kernel_name: python3
kernel_path: content
has_widgets: false
title: |-
  Developing NGBoost
pagenum: 6
prev_page:
  url: /4-saving.html
next_page:
  url: 
suffix: .ipynb
search: score distribution ngboost method class scores log metric implement must laplace not parameters should b fisher e mu new attribute scipy y censored want distributions sample user implementation logscore default implementing subclass also internal instance example stats our current data using laplacelogscore specific derive work here appropriate methods fit called regression facing need attributes respect wolframalpha information examples g deriving yet through process adding compatible distributional params mean well frac x mathbb r reference object vector return samples taking implemented its get value given dscore riemannian implmentation care www com input logfeebee cx acfeeb gradient thats test into making analysis

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Developing NGBoost</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you work with NGBoost, you may want to experiment with distributions or scores that are not yet supported. Here we will walk through the process of implementing a new distribution or score.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Adding-Distributions">Adding Distributions<a class="anchor-link" href="#Adding-Distributions"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The first order of business is to write the class for your new distribution. The distribution class must subclass the appropriate distribution type (either <code>RegressionDistn</code> or <code>ClassificationDistn</code>) and must implement methods for <code>fit()</code> and <code>sample()</code>. The scores compatible with the distribution should be stored in a class attribute called <code>score</code> and the number of parameters in an class attribute n_params. The class must also store the (internal) distributional parameters in a <code>_params</code> instance attribute. Additionally, regression distributions must implement a <code>mean()</code> method to support point prediction.</p>
<p>We'll use the Laplace distribution as an example. The Laplace distribution has PDF $\frac{1}{2b} e^{-\frac{|x-\mu|}{b}}$ with user-facing parameters $\mu \in \mathbb{R}$ and $b &gt; 0$, which we will call <code>loc</code> and <code>scale</code> to conform to the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.laplace.html"><code>scipy.stats</code> implementation</a>.</p>
<p>In NGBoost, all parameters must be represented internally in $\mathbb R$, so we need to reparametrize $(\mu, b)$ to, for instance, $(\mu, \log(b))$. The latter are the parameters we need to work with when we initialize a <code>Laplace</code> object and when implement the score.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="k">import</span> <span class="n">laplace</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">ngboost.distns.distn</span> <span class="k">import</span> <span class="n">RegressionDistn</span>
<span class="kn">from</span> <span class="nn">ngboost.scores</span> <span class="k">import</span> <span class="n">LogScore</span>

<span class="k">class</span> <span class="nc">LaplaceLogScore</span><span class="p">(</span><span class="n">LogScore</span><span class="p">):</span> <span class="c1"># will implement this later</span>
    <span class="k">pass</span>

<span class="k">class</span> <span class="nc">Laplace</span><span class="p">(</span><span class="n">RegressionDistn</span><span class="p">):</span>

    <span class="n">n_params</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">LaplaceLogScore</span><span class="p">]</span> <span class="c1"># will implement this later</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="c1"># save the parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span> <span class="o">=</span> <span class="n">params</span>
        
        <span class="c1"># create other objects that will be useful later</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loc</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logscale</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># since params[1] is log(scale)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">dist</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">Y</span><span class="p">):</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="c1"># use scipy&#39;s implementation</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">s</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span> <span class="c1"># gives us access to Laplace.mean() required for RegressionDist.predict()</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;loc&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">loc</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>fit()</code> method is a class method that takes a vector of observations and fits a marginal distribution. Meanwhile, <code>sample()</code> should return a $m$ samples from $P(Y|X=x)$, each of which is a vector of <code>len(Y)</code>.</p>
<p>Here we're taking advantage of the fact that <code>scipy.stats</code> already has the Laplace distribution implemented so we can steal its <code>fit()</code> method and put a thin wrapper around <code>rvs()</code> to get samples. We also use <code>__getattr__()</code> on the internal <code>scipy.stats</code> object to get access to its <code>mean()</code> method.</p>
<p>Lastly, we write a convenience method <code>params()</code> that, when called, returns the distributional parameters as the user expects to see them, i.e. $(\mu, b)$, not $(\mu, \log b)$.</p>
<h3 id="Implementing-a-Score-for-our-Distribution">Implementing a Score for our Distribution<a class="anchor-link" href="#Implementing-a-Score-for-our-Distribution"> </a></h3><p>Now we turn our attention to implementing a score that we can use with this distribution. We'll use the log score as an example.</p>
<p>All implemented scores should subclass the appropriate score and implement three methods:</p>
<ul>
<li><code>score()</code> : the value of the score at the current parameters, given the data <code>Y</code></li>
<li><code>d_score()</code> : the derivative of the score at the current parameters, given the data <code>Y</code></li>
<li><code>metric()</code> : the value of the Riemannian metric at the current parameters</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LaplaceLogScore</span><span class="p">(</span><span class="n">LogScore</span><span class="p">):</span> 
    
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">d_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># first col is dS/d𝜇, second col is dS/d(log(b))</span>
        <span class="n">D</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logscale</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="n">D</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logscale</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="k">return</span> <span class="n">D</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that the attributes of an instance of <code>Laplace</code> are referenced using the <code>self.attr</code> notation even though we haven't said these will be attributes of the <code>LaplaceLogScore</code> class. When a user asks NGBoost to use the <code>Laplace</code> distribution with the <code>LogScore</code>, NGBoost will first find the implmentation of the log score that is compatible with <code>Laplace</code>, i.e. <code>LaplaceLogScore</code> and dynamically create a new class that has both the attributes of the distribution and the appropriate implementation of the score. For this to work, the distribution class <code>Laplace</code> must have a <code>scores</code> class attribute that includes the implementation <code>LaplaceLogScore</code> and <code>LaplaceLogScore</code> must subclass <code>LogScore</code>. As long as those conditions are satisfied, NGBoost can take care of the rest.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The derivatives with respect to <a href="https://www.wolframalpha.com/input/?i=d%2Fdb+-log%281%2F%282e%5Eb%29+e%5E%28-%7Cx-a%7C%2Fe%5Eb%29%29">$\log b$</a> and <a href="https://www.wolframalpha.com/input/?i=d%2Fda+-log%281%2F%282e%5Eb%29+e%5E%28-%7Cx-a%7C%2Fe%5Eb%29%29">$\mu$</a> are easily derived using, for instance, WolframAlpha.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this example we won't bother implementing <code>metric()</code>, which would return the current Fisher Information. The reason is that the NGBoost implmentation of <code>LogScore</code> has a default <code>metric()</code> method that uses a Monte Carlo method to approximate the Fisher Information using the <code>gradient()</code> method and the distribution's <code>sample()</code> method (that's why we needed to implement <code>sample()</code>). By inhereting from <code>LogScore()</code>, not only can NGBoost find our implementation for the Laplace distribution, it can also fall back on the defualt <code>metric()</code> method. More on that later.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Putting it all together:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LaplaceLogScore</span><span class="p">(</span><span class="n">LogScore</span><span class="p">):</span> 
    
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">d_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># first col is dS/d𝜇, second col is dS/d(log(b))</span>
        <span class="n">D</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logscale</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="n">D</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logscale</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="k">return</span> <span class="n">D</span>

<span class="k">class</span> <span class="nc">Laplace</span><span class="p">(</span><span class="n">RegressionDistn</span><span class="p">):</span>

    <span class="n">n_params</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">LaplaceLogScore</span><span class="p">]</span> 

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="c1"># save the parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_params</span> <span class="o">=</span> <span class="n">params</span>
        
        <span class="c1"># create other objects that will be useful later</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loc</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logscale</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># since params[1] is log(scale)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">dist</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">Y</span><span class="p">):</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="c1"># use scipy&#39;s implementation</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">s</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span> <span class="c1"># gives us access to Laplace.mean() required for RegressionDist.predict()</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;loc&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">loc</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we can test our method:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ngboost</span> <span class="k">import</span> <span class="n">NGBRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_boston</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">mean_squared_error</span>

<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_reg_train</span><span class="p">,</span> <span class="n">X_reg_test</span><span class="p">,</span> <span class="n">Y_reg_train</span><span class="p">,</span> <span class="n">Y_reg_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">ngb</span> <span class="o">=</span> <span class="n">NGBRegressor</span><span class="p">(</span><span class="n">Dist</span><span class="o">=</span><span class="n">Laplace</span><span class="p">,</span> <span class="n">Score</span><span class="o">=</span><span class="n">LogScore</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_reg_train</span><span class="p">,</span> <span class="n">Y_reg_train</span><span class="p">)</span>
<span class="n">Y_preds</span> <span class="o">=</span> <span class="n">ngb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_reg_test</span><span class="p">)</span>
<span class="n">Y_dists</span> <span class="o">=</span> <span class="n">ngb</span><span class="o">.</span><span class="n">pred_dist</span><span class="p">(</span><span class="n">X_reg_test</span><span class="p">)</span>

<span class="c1"># test Mean Squared Error</span>
<span class="n">test_MSE</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_preds</span><span class="p">,</span> <span class="n">Y_reg_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test MSE&#39;</span><span class="p">,</span> <span class="n">test_MSE</span><span class="p">)</span>

<span class="c1"># test Negative Log Likelihood</span>
<span class="n">test_NLL</span> <span class="o">=</span> <span class="o">-</span><span class="n">Y_dists</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">Y_reg_test</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test NLL&#39;</span><span class="p">,</span> <span class="n">test_NLL</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[iter 0] loss=3.5592 val_loss=0.0000 scale=0.5000 norm=5.1998
[iter 100] loss=3.3162 val_loss=0.0000 scale=0.2500 norm=1.5259
[iter 200] loss=3.3106 val_loss=0.0000 scale=0.0312 norm=0.1790
[iter 300] loss=3.3104 val_loss=0.0000 scale=0.0000 norm=0.0001
[iter 400] loss=3.3104 val_loss=0.0000 scale=0.0000 norm=0.0001
Test MSE 61.24969569895786
Test NLL 3.2824343523391057
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Dig into the source of <code>ngboost.distns</code> to find more examples. If you write and test your own distribution, please contribute it to NGBoost by making a pull request!</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Censored-Scores">Censored Scores<a class="anchor-link" href="#Censored-Scores"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can make your distribution suitable for use in surival analysis if you implement a censored version of the score. The signature for the <code>score()</code>, <code>d_score()</code> and <code>metric()</code> methods should be the same, but they should expect <code>Y</code> to be indexable into two arrays like <code>E, T = Y["Event"], Y["Time"]</code>. Furthermore, any censored scores should be linked to the distribution class definition via a class attribute called <code>censored_scores</code> instead of <code>scores</code>.</p>
<p>Since censored scores are more general than their standard counterparts (fully observed data is a specific case of censored data), if you implement a censored score in NGBoost, it will automatically become available as a useable score for standard regression analysis. No need to implement the regression score seperately or register it in the <code>scores</code> class attribute.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Metrics">Metrics<a class="anchor-link" href="#Metrics"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we saw, using the log score, the easiest thing to do as a developer is to lean on the default ngboost method that calculates the log score metric.</p>
<p>However, the distribution-agnostic default method is slow because it must sample from the distribution many times to build up an approximation of the metric. If you want to make it faster, then you must derive and implement the distribution-specific Riemannian metric, which for the log score is the Fisher information matrix of that distribution. You have to derive the Fisher with respect to the internal ngboost parameterization (if that is different to the user-facing parametrization, e.g. $\log(\sigma)$, not $\sigma$). Deriving a Fisher is not necessarily easy since you have to compute an expectation analytically, but there are many examples onlne of deriving Fisher matrices that you can look through.</p>
<p>If you don't want to use the log score (say you want CRP score, for example), then ngboost does not (yet?) have a default method for calculating the metric and you <em>must</em> derive and implement it yourself. This is harder than deriving a Fisher because there are not many worked examples. The most general derivation process should follow the outline <a href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/">here</a>, replacing the KL divergence (which is induced by the log score) with whichever divergence is induced by the scoring rule you want to use (e.g. L2 for CRPS), again taking care to derive with respect to the internal ngboost parameterization, not the user-facing one. For any particular score, there may be a specific closed-form expression that you can use to calculate the metric across distributions (the expression for the Fisher Info serves this purpose for the log score) or there may not be- I actually don't know the answer to this question! But if there were, that could suggest some kind of default implementation for that score's <code>metric()</code> method.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Adding-Scores">Adding Scores<a class="anchor-link" href="#Adding-Scores"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We've seen how to <em>implement</em> an existing score for a new distribution, but making a new score altogether in NGBoost is also easy: just make a new class that subclasses <code>Score</code>:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ngboost.scores</span> <span class="k">import</span> <span class="n">Score</span>

<span class="k">class</span> <span class="nc">SphericalScore</span><span class="p">(</span><span class="n">Score</span><span class="p">):</span>
    <span class="k">pass</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That's it. Distribution-specific implemenations of this score (e.g. <code>LaplaceSphericalScore</code>) should subclass <code>SphericalScore</code>. The implementations of <code>LogScore</code> and <code>CRPScore</code> are in <code>ngboost.scores</code> for reference.</p>

</div>
</div>
</div>
</div>

 


    </main>
    